{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c6b36f",
   "metadata": {},
   "source": [
    "# 9 ARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7247a2",
   "metadata": {},
   "source": [
    "- Exponential Smoothing and ARIMA models are the two most widely used approached to Time Series forecasting.\n",
    "- its provides complementary approaches to the problem\n",
    "   - Exponential Smoothing models are based on description of the trend and seasonality in the data\n",
    "   - ARIMA models aim to describe the autocorrelations in the data\n",
    "   \n",
    "- ARIMA: Auto Regressive Integrated Moving Averages\n",
    "\n",
    "**Contents**\n",

    "    - Stationarity and differencing\n",
    "    - Backshift notion\n",
    "    - Autogressive models\n",
    "    - Moving Average models\n",
    "    - Non Seasonal ARIMA Models\n",
    "    - Estimation and orderd Selection\n",
    "    - Forecasting\n",
    "    - Seasonal ARIMA Models\n",
    "    - ARIMA vs ETS\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb07e0",
   "metadata": {},
   "source": [
    "## 9.1 Stationarity and Differncing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09632ff",
   "metadata": {},
   "source": [
    "### 9.1.1 Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd471e",
   "metadata": {},
   "source": [
    "- A time series is said to be stationary **if its statistical properties (mean, variance, autocorrelation) remain constant over time**. \n",
    "- In other words, if the data looks the same at any point in time, it is considered stationary.\n",
    "\n",
    "**Types of Stationarity**:\n",
    "\n",
    "- **Strict Stationarity**: A time series is strictly stationary if all its statistical properties remain constant over time, including the joint distribution of any subset of observations.\n",
    "- **Weak Stationarity**: A time series is weakly stationary if its **first two moments (mean and variance) remain constant over time**, and its autocovariance function depends only on the time lag between observations, not on the specific time points.\n",
    "\n",
    "\n",
    "**Why Stationarity Matters**:\n",
    "\n",
    "- Many statistical methods used in time series analysis assume stationarity.\n",
    "- Stationarity makes it easier to model and forecast time series data.\n",
    "- Non-stationary data can be made stationary through differencing.\n",
    "\n",
    "Remarks\n",
    "- **Thus, time series with trends, or with seasonality, are not stationary** — the trend and seasonality will affect the value of the time series at different times.\n",
    "- On the other hand, a white noise series is stationary\n",
    "- a time series with cyclic behaviour (but with no trend or seasonality) is stationary.\n",
    "   - This is because the cycles are not of a fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ed7e7",
   "metadata": {},
   "source": [
    "### 9.1.2 Differncing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeced69d",
   "metadata": {},
   "source": [
    "- Differencing is **a transformation applied to a time series to make it stationary**. \n",
    "- **It involves taking the difference between consecutive observations**.\n",
    "\n",
    "**Types of Differencing**:\n",
    "\n",
    "- **First-order differencing**: Taking the difference between consecutive observations.\n",
    "- Second-order differencing: Taking the difference between consecutive **first differences**.\n",
    "- Higher-order differencing: Taking the difference between consecutive **differences of the previous order**.\n",
    "\n",
    "**When to Use Differencing**:\n",
    "\n",
    "- When the time series shows a clear trend or seasonality.\n",
    "- When the autocorrelation function decays slowly or has a significant positive or negative value at large lags.\n",
    "\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- Consider a time series that shows a clear upward trend. First-order differencing can be applied to remove the trend, making the series stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab3c58",
   "metadata": {},
   "source": [
    "**Remarks**\n",
    "- Transformations such as logarithms can help to stabilise the variance of a time series. \n",
    "- Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality.\n",
    "- As well as the time plot of the data, the **ACF plot is also useful for identifying non-stationary time series**.\n",
    "\n",
    "   - Key Indicators in ACF Plots:\n",
    "\n",
    "     - Decay Rate: How quickly the correlation between observations diminishes as the lag increases.\n",
    "     - Pattern: The shape or pattern of the ACF plot.\n",
    "  \n",
    "  - **Stationary Time Series**:\n",
    "\n",
    "       - **Rapid Decay**: The ACF plot typically shows a rapid decay, meaning the correlation between observations decreases quickly as the lag increases.\n",
    "       \n",
    "       - **No Clear Pattern**: There's no discernible pattern or significant spikes at higher lags.\n",
    "\n",
    "  - **Non-Stationary Time Series**:\n",
    "\n",
    "    - **Slow Decay**: The ACF plot often shows a slower decay, indicating a persistent correlation between observations and past values.\n",
    "    - **Clear Pattern**: There may be a clear pattern, such as a repeating pattern or a slow decay.\n",
    "- **a large and positive r1 value** indicates a strong positive correlation between an observation and its immediate past, which is a common characteristic of **non-stationary time series**.\n",
    "\n",
    "  -first-order autocorrelation coefficient, r1, measures the correlation between a time series observation and its immediate past observation. It's calculated using the following formula:\n",
    "\n",
    "        r1 = Σ(yt - ȳ)(yt-1 - ȳ) / Σ(yt - ȳ)²\n",
    "where:\n",
    "\n",
    "       yt  : The value of the time series at time t\n",
    "       ȳ   : The mean of the time series\n",
    "       yt-1: The value of the time series at time t-1\n",
    "       Σ   : The summation operator \n",
    "  - Example:\n",
    "\n",
    "     - Consider a time series with the following values: 1, 2, 3, 4, 5.\n",
    "\n",
    "     - Mean: (1+2+3+4+5)/5 = 3\n",
    "     - Deviations: -2, -1, 0, 1, 2\n",
    "     - Products: (-2)(-1), (-1)(0), (0)(1), (1)(2) = 2, 0, 0, 2\n",
    "     - Sum of products: 2 + 0 + 0 + 2 = 4\n",
    "     - Variance: ((-2)² + (-1)² + 0² + 1² + 2²) = 10\n",
    "     - r1: 4 / 10 = 0.4\n",
    " - **Note**: A value of r1 close to 1 or -1 indicates a strong positive or negative correlation, respectively. A value close to 0 indicates a weak or no correlation.\n",
    " - **In practice, it is almost never necessary to go beyond second-order differences**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da84930",
   "metadata": {},
   "source": [
    "### 9.1.3 Seasonal Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb3946",
   "metadata": {},
   "source": [
    "- Seasonal differencing is a technique used in time series analysis to remove seasonal patterns from the data. \n",
    "- It involves subtracting the values of a time series from corresponding values in a previous period, typically a year.\n",
    "\n",
    "**How Seasonal Differencing Helps Make Time Series Stationary**:\n",
    "\n",
    "- Removes Seasonal Patterns: Seasonal differencing directly removes the seasonal component from the time series. This is particularly useful when there are recurring patterns that occur at regular intervals (e.g., monthly, quarterly, yearly).\n",
    "- Makes Data More Stationary: By eliminating the seasonal component, the remaining series is often more likely to exhibit stationarity. This is because the seasonal patterns can introduce non-stationarity, as the mean and variance of the series may vary over time due to these recurring patterns.\n",
    "- Improves Forecasting: Stationarity is a desirable property for many time series forecasting models. By making the data more stationary, seasonal differencing can improve the accuracy of forecasts.\n",
    "\n",
    "**Example**\n",
    "\n",
    "- If you have monthly data and want to remove yearly seasonality, you would subtract the value for this month last year from the value for this month this year. This would create a new series that is seasonally differenced.\n",
    "\n",
    "**Note**: In some cases, multiple levels of differencing may be necessary to achieve stationarity. For example, if a time series has both a trend and seasonality, first differencing can remove the trend, and then seasonal differencing can remove the remaining seasonal pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ae2aa",
   "metadata": {},
   "source": [
    "## 9.1.4 Unit Root Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8d665",
   "metadata": {},
   "source": [
    "- Unit Root Tests are statistical tests used to determine if a time series is non-stationary. \n",
    "- **A unit root in a time series indicates that the series is non-stationary**. This means that the statistical properties of the series (mean, variance, autocorrelation) change over time.\n",
    "- These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb8c5b3",
   "metadata": {},
   "source": [
    "**Common Unit Root Tests**:\n",
    "  - **Augmented Dickey-Fuller (ADF test**  determine the presence of a unit root\n",
    "    - null hypothesis of the Augmented Dickey-Fuller (ADF) test is that the time series is non-stationary. \n",
    "  - **Phillips-Perron (PP) test**:  also assumes  the null hypothesis of a unit root.\n",
    "  - **KPSS Test: Null Hypothesis is Stationarity**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad60ee",
   "metadata": {},
   "source": [
    "## Not here -- -- -- -- --\n",
    "\n",
    "**Null Hypothesis and Alternative Hypothesis**\n",
    "\n",
    "\n",
    "**Null Hypothesis (H0)**:\n",
    "\n",
    "- A statement of \"no effect\" or \"no difference.\"\n",
    "- It is the **default assumption that there is no significant relationship or difference between variables**.\n",
    "- **In statistical testing, the goal is to gather evidence to reject or fail to reject this null hypothesis**.\n",
    "\n",
    "\n",
    "**Alternative Hypothesis (H1)**:\n",
    "\n",
    "- A statement that contradicts the null hypothesis.\n",
    "- It is the claim that there is a significant relationship or difference between variables.\n",
    "- It is the hypothesis that the researcher is trying to prove.\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- H0: There is no difference in average test scores between students who study for 5 hours and those who study for 10 hours.\n",
    "- H1: Students who study for 10 hours have a higher average test score than those who study for 5 hours.\n",
    "\n",
    "**In statistical testing**:\n",
    "\n",
    "- We assume the null hypothesis is true until there is sufficient evidence to reject it.\n",
    "- If the evidence is strong enough, we reject the null hypothesis in favor of the alternative hypothesis.\n",
    "- If the evidence is not strong enough, we fail to reject the null hypothesis.\n",
    "\n",
    "\n",
    "**Key Points**:\n",
    "\n",
    "- The null hypothesis is always a statement of \"no effect\" or \"no difference.\"\n",
    "- The alternative hypothesis is the opposite of the null hypothesis.\n",
    "- The goal of statistical testing is to gather evidence to either reject or fail to reject the null hypothesis.   \n",
    "\n",
    "## -- -- -- -- --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c48e5",
   "metadata": {},
   "source": [
    "## 9.3 Autoregressive (AR) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b991e34",
   "metadata": {},
   "source": [
    "- **Autoregressive (AR) models are a class of statistical models used to describe the relationship between a variable and its own past values**. \n",
    "\n",
    "- **They assume that the current value of a variable is a linear function of its past values, plus a random error term.**\n",
    "\n",
    "\n",
    "**AR(p) Model**:\n",
    "\n",
    "   - An AR(p) model is an autoregressive model of order p. It is defined as follows:\n",
    "\n",
    "           Yt = c + φ1Yt-1 + φ2Yt-2 + ... + φpYt-p + εt\n",
    "     - where:\n",
    "\n",
    "     - Yt: The current value of the variable at time t\n",
    "     - c: A constant term\n",
    "     - φ1, φ2, ..., φp: The model coefficients\n",
    "     - Yt-1, Yt-2, ..., Yt-p: The past values of the variable\n",
    "     - εt: A random error term\n",
    "\n",
    "\n",
    "**Key Points**:\n",
    "\n",
    "- The **order p determines the number of lagged values used to predict the current value**.\n",
    "- The coefficients φ1, φ2, ..., φp measure the strength and direction of the relationship between the current value and its past values.\n",
    "- The random error term εt captures the unexplained variation in the data.\n",
    "\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- An AR(1) model would be:\n",
    "\n",
    "   - Yt = c + φ1Yt-1 + εt\n",
    "   - This means that the current value of Yt is a linear combination of the previous value Yt-1 and a random error term.\n",
    "\n",
    "**Choosing the Order (p)**:\n",
    "\n",
    "- ACF and PACF Plots: Analyzing the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots can help determine the appropriate order for an AR model.\n",
    "\n",
    "- Information Criteria: Metrics like Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) can be used to compare models with different orders and select the best-fitting one.\n",
    "\n",
    "\n",
    "**Applications of AR Models**:\n",
    "\n",
    "- **Time Series Forecasting**: AR models are commonly used to forecast future values of a time series.\n",
    "- **Financial Modeling**: They are used in financial modeling to analyze stock prices, exchange rates, and other financial time series.\n",
    "- **Econometrics**: AR models are applied in econometrics to study economic variables and their relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b25ecd",
   "metadata": {},
   "source": [
    "### 9.3.1 Using ACF and PACF Plots to Determine AR Model Order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa862f",
   "metadata": {},
   "source": [
    "**Understanding ACF and PACF Plots**\n",
    "\n",
    "- **Autocorrelation Function (ACF)**: Measures the correlation between a time series observation and its lagged values.\n",
    "\n",
    "- **Partial Autocorrelation Function (PACF)**: Measures the direct correlation between a time series observation and a lagged value, controlling for the effects of intervening lags.\n",
    "\n",
    "- **Must watch** https://www.youtube.com/watch?v=MxIfXbeP_Yw\n",
    "\n",
    "**Interpreting ACF and PACF Plots for AR Models**:\n",
    "\n",
    "  - **Finding P value for AR models**:\n",
    "- ACF: Typically decays exponentially, with a significant spike at lag 1 and then decaying to zero. \n",
    "       \n",
    "    - it can downward with first positive value towards zero\n",
    "    - it can upward with first high negative value towards zero\n",
    "    - it can decrease in zigzag nature\n",
    "\n",
    "- PACF: Shows a significant spike at lag p and then decays to zero.\n",
    "\n",
    "- **Must watch** https://www.youtube.com/watch?v=_nSvoCkodS8&t=11s "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454d5f0",
   "metadata": {},
   "source": [
    "## -- -- -- -- --\n",
    "## 9.4 Moving Average Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8aec83",
   "metadata": {},
   "source": [
    "- A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values.\n",
    "\n",
    "**Moving Average (MA) models** are another class of time series models that assume the current value of a variable is a **linear combination of past error terms**. \n",
    "- They are often used in conjunction with Autoregressive (AR) models to form ARMA models.\n",
    "\n",
    "  - MA(q) Model: An MA(q) model is a Moving Average model of order q. It is defined as follows:\n",
    "\n",
    "        Yt = θ0 + εt + θ1εt-1 + θ2εt-2 + ... + θqεt-q\n",
    "where:\n",
    "\n",
    "     - Yt: The current value of the variable at time t\n",
    "     - θ0: A constant term\n",
    "     - εt, εt-1, ..., εt-q: Past error terms\n",
    "     - θ1, θ2, ..., θq: The model coefficients\n",
    "\n",
    "\n",
    "**Key Points**:\n",
    "\n",
    "- The order q determines the number of past error terms used to predict the current value.\n",
    "- The coefficients θ1, θ2, ..., θq measure the influence of past errors on the current value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8d70d",
   "metadata": {},
   "source": [
    "**Moving Average Smoothing**:\n",
    "\n",
    "Moving Average smoothing is a technique used to create a smoother version of a time series. \n",
    "  - It involves calculating the average of a specified number of consecutive observations.\n",
    "  - It used for estimating the trend-cycle of past values.\n",
    "\n",
    "**Difference between MA Models and Moving Average Smoothing:\n",
    "\n",
    "- **Purpose**: MA models are used to model the relationship between a variable and its past errors, while moving average smoothing is used to smooth out noise and identify trends in the data.\n",
    "- **Calculation**: MA models use specific coefficients to weight past errors, while moving average smoothing simply takes the average of a fixed number of observations.\n",
    "- **Interpretation**: MA models provide a statistical model for the underlying process, while moving average smoothing is a data-driven technique for visualization and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133f898",
   "metadata": {},
   "source": [
    "**Invertible Nature of MA Models**:\n",
    "\n",
    "- An MA model is said to be invertible if it can be expressed as an equivalent AR model. \n",
    "- This means that the past values of the series can be expressed as a function of the current and past error terms.\n",
    "\n",
    "  - **Invertibility**: If an MA model is invertible, it can be rewritten as an AR model.\n",
    "  - **Non-Invertibility**: If an MA model is not invertible, it cannot be expressed as an AR model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e74d6ec",
   "metadata": {},
   "source": [
    "### 9.4.1 Determining the Order (q) of a Moving Average (MA) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b6d11",
   "metadata": {},
   "source": [
    "To find the appropriate value of q, you can use the following methods:\n",
    "- **1. ACF and PACF Plots**\n",
    "   - ACF: For an MA(q) model, the ACF will cut off after lag q and the PACF will decay exponentially.\n",
    "   - PACF: The PACF for an MA(q) model will decay exponentially.\n",
    "   - **Must watch** https://www.youtube.com/watch?v=a0BVTH86JrI\n",
    "   - Steps:\n",
    "\n",
    "      - **Examine the ACF plot: Identify the lag at which the ACF cuts off or becomes negligible**.\n",
    "      - **Examine the PACF plot: Look for a clear exponential decay in the PACF**.\n",
    "      - Compare ACF and PACF: If the ACF cuts off after lag q and the PACF decays exponentially, then an MA(q) model is likely appropriate.\n",
    "\n",
    "- 2. **Information Criteria**\n",
    "  - **AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)**: These metrics can be used to compare models with different orders and select the best-fitting one.\n",
    "  - **Lower AIC or BIC**: Generally, a lower AIC or BIC value indicates a better-fitting model.\n",
    "- 3. **Subject Matter Knowledge**\n",
    "   - Consider the underlying process and any known characteristics of the data when selecting the order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a076a",
   "metadata": {},
   "source": [
    "## 9.5 Non-Seasonal ARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242f692",
   "metadata": {},
   "source": [
    "**ARIMA (AutoRegressive Integrated Moving Average)** models are a class of statistical models used to represent time series data. \n",
    "- They **combine Autoregressive (AR) and Moving Average (MA) components to capture both autoregressive and moving average effects in the data**.\n",
    "\n",
    "\n",
    "**ARIMA(p,d,q) Model**:\n",
    "\n",
    "  - p: The order of the autoregressive part, representing the number of lagged values of the dependent variable used as predictors.\n",
    "  - d: The degree of differencing, indicating the number of times the data needs to be differenced to make it stationary.\n",
    "  - q: The order of the moving average part, representing the number of lagged error terms used as predictors.\n",
    "\n",
    "\n",
    "**Non-Seasonal ARIMA Model**:\n",
    "\n",
    "A non-seasonal ARIMA model assumes that there is no seasonal component in the data. It is defined as:\n",
    "\n",
    "      (Yt - ϕ1Yt-1 - ... - ϕpYt-p) = θ0 + εt + θ1εt-1 + ... + θqεt-q\n",
    "where:\n",
    "\n",
    "   - Yt: The current value of the variable at time t\n",
    "   - ϕ1, ϕ2, ..., ϕp: The AR coefficients\n",
    "   - θ0, θ1, ..., θq: The MA coefficients\n",
    "   - εt: The error term at time t\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709ca28a",
   "metadata": {},
   "source": [
    "  \n",
    "**Steps to Build a Non-Seasonal ARIMA Model**:\n",
    "\n",
    " - **Stationarity**: Ensure the time series is stationary. If not, apply differencing (d) to make it stationary.\n",
    " - **Model Identification**: Use ACF and PACF plots to identify the appropriate values for p and q.\n",
    " - **Model Estimation**: Estimate the model parameters using a suitable method, such as maximum likelihood estimation.\n",
    " - **Model Diagnostics**: Assess the model's fit using diagnostic tests, such as the Ljung-Box test for autocorrelation in the residuals.\n",
    "- **Forecasting**: Use the estimated model to forecast future values of the time series.\n",
    "\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- ARIMA(1,1,1): This model suggests a \n",
    "  - first-order autoregressive component, \n",
    "  - first-order differencing, and a \n",
    "  - first-order moving average component.\n",
    "**Key Points**:\n",
    "\n",
    "- Non-seasonal ARIMA models are suitable for time series data without a clear seasonal pattern.\n",
    "- The values of p, d, and q are chosen based on the characteristics of the data and the model's performance.\n",
    "- Model diagnostics are crucial to ensure the model's validity and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b30e74",
   "metadata": {},
   "source": [
    "**AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion)** are commonly used metrics to compare the fit of statistical models, including ARIMA models.\n",
    "\n",
    "**Lower is better**.\n",
    "\n",
    "- AIC and BIC penalize models with more parameters. \n",
    "  - This helps to **prevent overfitting**, where a model becomes too complex and fits the training data too closely, potentially leading to poor performance on new data.\n",
    "- A lower AIC or BIC value indicates a better balance between model fit and complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1742f7",
   "metadata": {},
   "source": [
    "**Important Notes**\n",
    "- ACF and PACF are useful for identifying p in ARIMA(p, d, 0) models and q in ARIMA(0, d, q) models\n",
    "- **When both p and q are positive, the behavior of the ACF and PACF plots can become more complex and intertwined**.\n",
    "   - Neither the ACF nor the PACF will have a clear cutoff, as seen in pure AR or MA processes.\n",
    "   - In such cases, it’s harder to directly determine the values of p and q using just the ACF and PACF plots.\n",
    "   \n",
    "- Instead, you may need to rely on:\n",
    "\n",
    "  - **Grid search or information criteria (like AIC, BIC)** to find the best combination of p and q.\n",
    " - **Auto ARIMA** (automated selection methods) can help to determine both p and q when both terms are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c02ecb",
   "metadata": {},
   "source": [
    "#### Imp Notes Revision\n",
    "\n",
    "- The data may follow an ARIMA( p,d,0) model if the ACF and PACF plots of the differenced data show the following patterns:\n",
    "  - the ACF is exponentially decaying or **sinusoidal**; \n",
    "  - there is a significant spike at lag p in the PACF, but none beyond lag p\n",
    " .\n",
    "- The data may follow an ARIMA(0, d,q) model if the ACF and PACF plots of the differenced data show the following patterns:\n",
    "\n",
    "  - the PACF is exponentially decaying or sinusoidal;\n",
    "  - there is a significant spike at lag q in the ACF, but none beyond lag q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e92a54",
   "metadata": {},
   "source": [
    "| Model Type                | ARIMA Representation                |\n",
    "|---------------------------|-------------------------------------|\n",
    "| White noise               | ARIMA(0,0,0) with no constant       |\n",
    "| Random walk               | ARIMA(0,1,0) with no constant       |\n",
    "| Random walk with drift     | ARIMA(0,1,0) with a constant        |\n",
    "| Autoregression            | ARIMA(p,0,0)                       |\n",
    "| Moving average            | ARIMA(0,0,q)                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae04e6",
   "metadata": {},
   "source": [
    "- Integration in ARIMA models refers to the **reverse process of differencing**. It essentially involves \"undoing\" the differencing operations to obtain the original time series.\n",
    "\n",
    "#### -- -- -- -- --\n",
    "\n",
    "**ARIMA Model Components: AR, I, and MA**\n",
    "\n",
    "ARIMA (AutoRegressive Integrated Moving Average) models are a combination of three components: Autoregressive (AR), Integrated (I), and Moving Average (MA). Let's break down what each part does:\n",
    "\n",
    "- **AR (Autoregressive) Component**\n",
    "   - Purpose: Models the relationship between the current value of a time series and its own past values.\n",
    "   - How it works: Uses a linear combination of previous values to predict the current value.\n",
    "   - Example: An AR(1) model would use the previous value to predict the current value.\n",
    "\n",
    "- **I (Integrated) Component**\n",
    "   - Purpose: Makes a non-stationary time series stationary by differencing.\n",
    "   - How it works: The order of integration (d) indicates how many times the data needs to be differenced to become stationary.\n",
    "   - Example: If d=1, the first difference is taken. If d=2, the second difference is taken.\n",
    "   \n",
    "   \n",
    "- **MA (Moving Average) Component**\n",
    "  - Purpose: Models the relationship between the current value of a time series and past error terms.\n",
    "  - How it works: Uses a linear combination of past error terms to predict the current value.\n",
    "  - Example: An MA(1) model would use the previous error term to predict the current value.\n",
    "\n",
    "**In summary**:\n",
    "\n",
    "  - AR: Models the dependence on past values of the time series itself.\n",
    "  - I: Makes the series stationary by differencing.\n",
    "  - MA: Models the dependence on past errors.\n",
    "     - Combining these components: **ARIMA(p,d,q)**:\n",
    "        - p: Order of the AR component\n",
    "        - d: Order of the integrated component\n",
    "        - q: Order of the MA component\n",
    "  - By combining these components, ARIMA models can capture a wide range of patterns in time series data, including trends, seasonality, and autocorrelation.\n",
    "  \n",
    "#### -- -- -- -- --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbc584",
   "metadata": {},
   "source": [
    "- MLE is a powerful method for estimating the parameters of time series models.\n",
    "- It seeks to find the set of parameters that maximizes the likelihood of observing the given data.\n",
    "\n",
    "\n",
    "**Key Steps**:\n",
    "\n",
    "- **Specify the Model**: Choose a suitable time series model (e.g., ARIMA, SARIMA, GARCH) based on the characteristics of the data.\n",
    "- Write the Likelihood Function: Derive the likelihood function for the chosen model. This function expresses the probability of observing the data given the model parameters.\n",
    "- **Maximize the Likelihood**: Find the values of the model parameters that maximize the likelihood function. This can be done using optimization techniques like numerical optimization algorithms.\n",
    "- **Evaluate the Model**: Assess the goodness of fit of the estimated model using diagnostic tests and information criteria.\n",
    "\n",
    "\n",
    "**Advantages of MLE**:\n",
    "\n",
    "- **Efficiency**: MLE provides efficient estimates under certain conditions.\n",
    "- **Flexibility**: It can be applied to a wide range of time series models.\n",
    "- **Statistical Inference**: MLE allows for statistical inference, such as hypothesis testing and confidence interval construction.\n",
    "\n",
    "\n",
    "**Disadvantages of MLE**:\n",
    "\n",
    "- **Computational Complexity**: MLE can be computationally intensive for complex models or large datasets.\n",
    "- **Sensitivity to Initial Values**: The optimization algorithm may converge to different solutions depending on the initial parameter values.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
