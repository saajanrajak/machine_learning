{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0814c670",
   "metadata": {},
   "source": [
    "# Contents \n",
    "- what are Ensemble Methods\n",
    "- why these are popular\n",
    "- what are different types of it\n",
    "\n",
    "  - Votting Classifiers\n",
    "- Bagging and Pasting\n",
    "  - Out-of-bag evaluation\n",
    "  - Random Patches and Random Subspaces\n",
    "- Random Forests\n",
    "  - Extra Tree\n",
    "  - Feature Importance\n",
    "- Boosting \n",
    "- Stacking\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71fdad8",
   "metadata": {},
   "source": [
    "# 1. Ensemble Learning and Ensemble Method\n",
    "\n",
    "- **Ensemble learning** is machine learning approach where multiple models (often called **weak learners**) are trained and combined to predict to solve the same problem, with aim of imporving the overall performance compared to single model\n",
    "\n",
    "\n",
    "\n",
    "- **Ensemble Methods** are the specific methods used to implment ensemble learnings. Types of ensemble methods are\n",
    "   - **Bagging** (also know as Bootstrap Aggregation): Create multiple models by training them on different random subsets of the data created via bootstrap sampling\n",
    "      - **bootstrap sampling** (sampling with replacement).\n",
    "      - In classification, bagging combines predictions via majority or weighted voting\n",
    "      - In regression, bagging combines predictions via averaging.\n",
    "      - Random Forest is a popular example of bagging applied to decision trees, and it uses these aggregation methods for final predictions.\n",
    "   \n",
    "   - **Boosting**: Builds Models Sequentially, with each model attempting to correct the errors of the previous one\n",
    "   \n",
    "   - **Stacking**: Combines the predications of multiple models using another model (called **metalearner**) to make final decisions\n",
    "   \n",
    "   - **Voting**: Aggregates predications by having each model(**base learners**) 'vote', and the final decision is based on majority or weighted votes\n",
    "     - trained on same dataset (no different subsets) \n",
    "     - base learers/models are often of differnt type ( one model can be regression, another svm, another decision tree)\n",
    "     - Voting focuses on combining the strengths of different models to make better predictions\n",
    "   \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638d67a",
   "metadata": {},
   "source": [
    "# 2. Voting Clasifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa431be9",
   "metadata": {},
   "source": [
    "- an ensemble method that combines the predictions of multiple different models (often called \"base learners\") to improve the overall accuracy. \n",
    "- The idea is to \"vote\" on the final prediction by aggregating the individual predictions from each model.\n",
    "\n",
    "- **Types of Voting Classifiers**\n",
    "    - **Hard Voting Classifier** : Each model makes predication and the final predication is defined by majority voting\n",
    "    - the class gets the major votes is choose as the final predications\n",
    "      - ex, 3 models predicts (Class A, Class B, Class A) , final predication will be **Class A**\n",
    "      \n",
    "    - **Soft Voting Classifier**: Instead of predicating just the class, each model outputs the probability of each class\n",
    "    - the final predications is based on the **average of predicated probabities of each class**\n",
    "    - the class with highest probablities will be selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7830da",
   "metadata": {},
   "source": [
    "- voting classifier often achieves a higher accuracy than the best classifier in the ensemble\n",
    "\n",
    "- In fact, even if each classifier is a weak learner (meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners in the ensemble and they are sufficiently diverse.\n",
    "\n",
    "- **Ensemble methods work best when the predictors are as independent from one another as possible.**\n",
    "- One way to get diverse classifiers is to train them using very different algorithms. \n",
    "- This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "587a3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets code: Voting classifier \n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X, y = make_moons(n_samples = 500, noise = 0.30, random_state = 42)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe52396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                             (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                             (&#x27;svc&#x27;, SVC(random_state=42))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                             ('rf', RandomForestClassifier(random_state=42)),\n",
       "                             ('svc', SVC(random_state=42))])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(estimators = [('lr', LogisticRegression(random_state = 42)),\n",
    "                                            ('rf', RandomForestClassifier(random_state = 42)),\n",
    "                                            ('svc', SVC(random_state = 42))\n",
    "                                            \n",
    "                                           ])\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c29055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.864\n",
      "rf = 0.896\n",
      "svc = 0.896\n"
     ]
    }
   ],
   "source": [
    "# lets check accuracy from each fitted classifier\n",
    "\n",
    "for name, clf in voting_clf.named_estimators_.items():\n",
    "    print(name, \"=\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349689ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##lets use voting clf to predict,\n",
    "## it performs hard voting\n",
    "\n",
    "voting_clf.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b090e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1]), array([1]), array([0])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.predict(X_test[:1]) for clf in voting_clf.estimators_]\n",
    "\n",
    "# voting classifier predicts class 1 for the first instance of X_test \n",
    "# because, below, out of three classifiers, first two predicat class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "312e93d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets look performance of the voting classifier on test set\n",
    "\n",
    "voting_clf.score(X_test, y_test)\n",
    "\n",
    "# The voting classifer outperforms all the individual classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457b16c7",
   "metadata": {},
   "source": [
    "**For Soft Voting**\n",
    "\n",
    " - we need to predict the probabilities of all different classifiers\n",
    " - it will only happen if all the classifiers have predict function is available\n",
    " - we will average the probabilities of the class, to find highes class via max avg value\n",
    " \n",
    " - **it oftens achieves higher performance than hard voting classifiers**\n",
    "   - because it gives more weight to highly confident votes\n",
    "   \n",
    " - we have to define votingclassifier **voting** hyperparmeter to soft\n",
    " - and ensure that all the classifiers can estimate the class probabilities\n",
    "    - for above set, SVC doesn't have capcity to predict probabiliteis by default, we need to set the hyperparameter to true\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe3b7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.voting = 'soft'\n",
    "voting_clf.named_estimators['svc'].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)\n",
    "\n",
    "# which is bit better than hard voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588db97b",
   "metadata": {},
   "source": [
    "# 3. Bagging and Pasting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3117c",
   "metadata": {},
   "source": [
    "- **Bagging**: (short for bootstrap aggregating) when sampling is performed with replacement\n",
    "   - bagging allows for the duplication of rows within a single subset due to its sampling method (sampling with replacement)\n",
    "      - This characteristic is what helps bagging to reduce variance and improve the performance of the ensemble model.\n",
    "- **Pasting** when sampling is perfomed without replacment\n",
    "- both Bagging and pasting allow training instances to be sampled several times across multiple predicators\n",
    "   - but only bagging allows training instances to be sampled sevaral times for the same preditor\n",
    "\n",
    "## ----\n",
    "**Sampling Without Replacement: When creating subsets in pasting, the method samples rows without replacement. This means that each row selected for a specific subset cannot be repeated within that same subset. However, it can be included in a different subset**   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b78f84",
   "metadata": {},
   "source": [
    "### <u>Not here</u>\n",
    "- **Difference b/w Bootstrap and Bagging**\n",
    "   - Bootstrap :  It is a sampling technique where random samples are drawn with replacement from a dataset.\n",
    "      - Purpose: It is used to estimate the distribution of a statistic by creating multiple datasets (called bootstrap samples) from the original dataset. These samples are then used to compute statistics like the mean, variance, or more complex model predictions.\n",
    "      \n",
    "   - Bagging: Bagging is an ensemble learning method that uses bootstrapping to improve model performance. It involves creating multiple models by training them on different bootstrapped datasets and then aggregating their predictions (usually by averaging for regression or voting for classification).\n",
    "   \n",
    "      - Purpose: The goal of bagging is to reduce model variance and improve overall accuracy by combining multiple weak models into a stronger, more stable model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58202294",
   "metadata": {},
   "source": [
    "### <u>Not Here</u>\n",
    "In decision trees, the objectives differ between classification and regression tasks. Here's a breakdown of each:\n",
    "\n",
    " **Decision Tree Objective**\n",
    "\n",
    "1. **For Classification**:\n",
    "   - **Objective**: To create the next node with the aim of achieving a **pure node**.\n",
    "     - A **pure node** is a node where all the observations belong to a single class. \n",
    "     - The goal is to minimize impurity (e.g., Gini impurity, entropy) in the nodes.\n",
    "     - The decision tree algorithm evaluates potential splits based on how well they separate the classes, seeking to maximize the homogeneity of the classes in the child nodes.\n",
    "\n",
    "2. **For Regression**:\n",
    "   - **Objective**: To create the next node with the aim of minimizing the **prediction error** (or variance) within the node.\n",
    "     - In regression trees, the aim is to reduce the variance of the target variable (e.g., the numerical output) in the nodes.\n",
    "     - The algorithm typically uses metrics like **Mean Squared Error (MSE)** or **Mean Absolute Error (MAE)** to evaluate splits.\n",
    "     - By minimizing the prediction error, the algorithm aims to make the predictions for the target variable as accurate as possible within each leaf node.\n",
    "\n",
    "**Summary**:\n",
    "- **Classification**: Aim for pure nodes (homogeneous classes).\n",
    "- **Regression**: Aim to minimize prediction error (homogeneity of target values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9599600",
   "metadata": {},
   "source": [
    "  ## 3.1 Bagging and Pasting in Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "385b7403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BaggingClassifier</label><div class=\"sk-toggleable__content\"><pre>BaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=1, random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=100,\n",
       "                  n_estimators=500, n_jobs=1, random_state=42)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                           n_estimators = 500,\n",
    "                           max_samples = 100,\n",
    "                           n_jobs = 1,\n",
    "                           random_state = 42)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e00e68",
   "metadata": {},
   "source": [
    "\n",
    "A BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a predict_proba() method), which is the case with decision tree classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e41ec",
   "metadata": {},
   "source": [
    "**Bagging Classifiers parameters**\n",
    "\n",
    "- **base_estimator**: estimator object, classifier name\n",
    "- **n_estimators** = 500 ::  This parameter defines the number of base estimators (classifiers) in the ensemble. 500 decision tree classifiers\n",
    "\n",
    "- **max_samples**, training sample size for each decision tree classifiers\n",
    "\n",
    "- **max_features**: This parameter defines the number of features to draw from the total number of features to train each base estimator. Similar to max_samples, if set as a float, it represents a fraction. For instance, max_features=0.5 means that each base estimator will use 50% of the features.\n",
    "   - deafult 1,\n",
    "- **bootstrap**: default=True\n",
    "   - This parameter indicates whether samples are drawn with replacement. If True, it allows the model to sample with replacement; if False, it samples without replacement (known as pasting).\n",
    "\n",
    "- **bootstrap_features** : default=False\n",
    "   - This parameter allows for bootstrapping features. If True, features are sampled with replacement. It’s often used for high-dimensional data.\n",
    "   \n",
    "- **n_job** : it tells Scikit-Learn the number of CPU cores to use for training and predictions, and –1 tells Scikit- Learn to use all available cores\n",
    "\n",
    "- **oob_score**: default=False\n",
    "  - If True, this parameter enables out-of-bag (OOB) evaluation, which provides an internal validation score by using the unused samples in the bootstrap samples. It can give insights into model performance without the need for a separate validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41dcfe",
   "metadata": {},
   "source": [
    "- **Bagging introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting** \n",
    "- but the extra diversity also means that the predictors end up being less correlated,so the ensemble’s variance is reduced. \n",
    "\n",
    "- Overall, **bagging often results in better models, which explains why it’s generally preferred**. \n",
    "- But if you have spare time and CPU power, you can use cross-validation to evaluate both bagging and pasting and select the one that works best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba09df",
   "metadata": {},
   "source": [
    "## 3.2 Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a1eb1",
   "metadata": {},
   "source": [
    "-  **Out-of-Bag (OOB) Evaluation** is a method used in ensemble learning, particularly with bagging algorithms like the BaggingClassifier in Scikit-learn. It provides a way to assess the performance of an ensemble model without the need for a separate validation set\n",
    "\n",
    "- The concept of out-of-bag instances and the average sampling rate of about 63% is fundamental to understanding how Bagging works\n",
    "\n",
    "- the remaining 37% of the training instances that are not sampled are called out-of-bag (OOB) instances. **Note that they are not the same 37% for all predictors/models/tree samples**\n",
    "\n",
    "**OOB Instances**:\n",
    "\n",
    "  - The instances that are not included in each bootstrap sample for a given estimator are considered its out-of-bag instances.\n",
    "- Since each estimator has its unique bootstrap sample, the OOB instances for Estimator 1 are different from those for Estimator 2. This leads to a different set of OOB instances for each base model.\n",
    "\n",
    "###  ----\n",
    "\n",
    "- we can set **\"oob_score=True\"** when creating a BaggingClassifier to request an automatic OOB evaluation after training.\n",
    "- The resulting evaluation score is available in the **oob_score_**  attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2a40f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n",
    "                               n_estimators = 500,\n",
    "                               oob_score = True,\n",
    "                               n_jobs = 1,\n",
    "                                random_state =42\n",
    "                               )\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f511fdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#according to above result, oob suggests we will likely to achieve about 89% accuracy in test data lets check\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb9fe41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another method to find accuracy score\n",
    "bag_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a7b8b7",
   "metadata": {},
   "source": [
    "**Not here** \n",
    "- Decision function is a crucial component in many classifiers, providing a numerical score that helps in determining class membership.\n",
    "\n",
    "- By understanding the decision function, one can interpret the model’s confidence in its predictions and make adjustments to thresholds as necessary for specific applications\n",
    "\n",
    "  - For binary classifiers: A common threshold is 0, where scores greater than 0 indicate one class, and scores less than 0 indicate the other.\n",
    "   - For multi-class classifiers: The class with the highest score is typically chosen as the predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abfa4ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32352941, 0.67647059],\n",
       "       [0.3375    , 0.6625    ],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check oob_decision_function \n",
    "\n",
    "bag_clf.oob_decision_function_[:3] #probas of the first 3 instance of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974dac2",
   "metadata": {},
   "source": [
    "## 3.3 Random Patches and Random Subspaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1ce53",
   "metadata": {},
   "source": [
    "- Random Patches and Random Subspaces are two advanced ensemble techniques that are variations of bagging. They both **aim to enhance the diversity among the individual learners in an ensemble model, improving generalization performance**.\n",
    "\n",
    "#### ------\n",
    "- **Random Patches**: its a method combines two techniques: bootstrapping (sampling training instances with replacement) and feature selection. In this method, each base learner is **trained on a random subset of both the training instances and the features**\n",
    "   - diversity in both instances and features helps to reduce overfitting and improves the model's robustness.\n",
    "   - seful for high-dimensional datasets (such as images) where feature selection can significantly impact model performance\n",
    "   \n",
    "\n",
    "- **Random Subspaces** method specifically focuses on the **random selection of features while using the entire training dataset**. Each base learner is trained on the same set of instances but with a different subset of features\n",
    "   - this method emphasizes increasing diversity among the models by varying the feature sets while maintaining the full dataset.\n",
    "   - It helps to reduce correlation between individual estimators in the ensemble.\n",
    "   \n",
    "   \n",
    "## -----\n",
    "\n",
    "- Sampling features (by setting bootstrap_features to True and/or max_features to a value smaller than 1.0\n",
    "- Keeping all training instances (by setting bootstrap=False and max_samples=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bcf2fe",
   "metadata": {},
   "source": [
    "# 4. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fa0651",
   "metadata": {},
   "source": [
    "- Random Forest is an ensemble method that can do both classfication and regression.\n",
    "- it builds multiple decision tree during training and outputs the mode of classes (for classfication) and average(for regression) of the individual trees\n",
    "\n",
    "**Random Forest works**\n",
    "\n",
    "- Bootstrap Sampling: also know as bagging\n",
    "   - create random samples/subsets of datasets with replacement\n",
    "- Random Feature Selection: \n",
    "   - at each node, random forest select randomly a set of feature instead of considering all features to find the best split\n",
    "- Multiple Decison tree:\n",
    "   - Multiple DT are trained independentally on these random subsets of data and features.\n",
    "- Aggregation of Predications:\n",
    "   - Classfication : majority vote\n",
    "   - Regression: Average of the predications\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "510533dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, \n",
    "                                max_leaf_nodes = 16,\n",
    "                                 n_jobs = -1,\n",
    "                                 random_state = 42)\n",
    "\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "# print score\n",
    "rnd_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a6974b",
   "metadata": {},
   "source": [
    "\n",
    "- **With a few exceptions, a RandomForestClassifier has all the hyperparameters of a DecisionTreeClassifier (to control how trees are grown)**, plus all the hyperparameters of a BaggingClassifier to control the ensemble itself.\n",
    "\n",
    "- The random forest algorithm introduces extra randomness when growing trees; **instead of searching for the very best feature when splitting a node , it searches for the best feature among a random subset of features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90c48e",
   "metadata": {},
   "source": [
    "## 4.1 Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653da877",
   "metadata": {},
   "source": [
    "**Extra Trees (short for \"Extremely Randomized Trees\")** \n",
    "- it is a variant of the Random Forest algorithm, and it's part of the broader family of ensemble methods. \n",
    "- While it shares many similarities with Random Forest, there are a few key differences that make Extra Trees more \"random\" than Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b281cbf",
   "metadata": {},
   "source": [
    "| Feature               | Random Forest                                             | Extra Trees                                               |\n",
    "|-----------------------|-----------------------------------------------------------|-----------------------------------------------------------|\n",
    "| **Bootstrap Sampling** | Yes (by default)                                          | No (uses entire dataset unless `bootstrap=True`)           |\n",
    "| **Splitting Criterion**| Finds the best split for each node (greedy search)        | Randomly selects the split threshold                       |\n",
    "| **Diversity among Trees** | Diversity comes from bootstrapped samples and random feature selection | Diversity comes from random splits and random feature selection |\n",
    "| **Training Speed**     | Slower (due to finding optimal splits)                    | Faster (no search for best splits)                         |\n",
    "| **Bias-Variance Tradeoff** | Lower bias, higher variance                              | Higher bias, lower variance                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9cccc",
   "metadata": {},
   "source": [
    "- Both methods are very effective, and often the best choice depends on your specific dataset and the trade-offs between speed and performance.\n",
    "\n",
    "-  **Both technique trades more bias for a lower variance**\n",
    "- We can create an extra-trees classifier using Scikit-Learn’s **ExtraTreesClassifier** class\n",
    "  -  Its API is identical to the RandomForestClassifier class, except bootstrap defaults to False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb71db",
   "metadata": {},
   "source": [
    "## 4.2 Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50395ce0",
   "metadata": {},
   "source": [
    "- Random Forest has a great quality, it make easy to measure the relative importance of each feature.\n",
    "\n",
    "\n",
    "- **Random Forest computes feature importance based on how much a feature reduces the impurity (like Gini impurity or entropy in classification) or how much it reduces the variance in regression**\n",
    "\n",
    "\n",
    "   - More precisely, it is a weighted average, where each node’s weight is equal to the number of training samples that are associated with it\n",
    "   \n",
    "\n",
    "- Scikit-Learn computes this score automatically for each feature after training, **then it scales the results so that the sum of all importances is equal to 1**\n",
    "   - Interpretation: Higher values mean that a feature contributes more to the predictions\n",
    "   - Model-Specific: Feature importance in Random Forest is specific to the model; other algorithms may give different rankings\n",
    "\n",
    "- **We can access the result using the feature_importances_**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f4bc462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11 sepal length (cm)\n",
      "0.02 sepal width (cm)\n",
      "0.44 petal length (cm)\n",
      "0.42 petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris(as_frame = True)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, # build 500 Tree\n",
    "                           random_state = 42)\n",
    "\n",
    "rnd_clf.fit(iris.data, iris.target)\n",
    "\n",
    "for score, name in zip(rnd_clf.feature_importances_, iris.data.columns):\n",
    "    print(round(score,2), name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfeb64",
   "metadata": {},
   "source": [
    "**Key Differnce Between Bagging and Boosting**\n",
    "\n",
    "| **Aspect**            | **Bagging**                                               | **Boosting**                                             |\n",
    "|-----------------------|-----------------------------------------------------------|----------------------------------------------------------|\n",
    "| **Primary Focus**      | Reducing variance (overfitting)                           | Reducing bias (underfitting)                             |\n",
    "| **Model Type**         | Suitable for high-variance models (e.g., Decision Trees)  | Suitable for high-bias models (e.g., weak learners)       |\n",
    "| **Training**           | Models are trained independently                          | Models are trained sequentially, each improving on the previous |\n",
    "| **Risk of Overfitting**| Lower risk of overfitting (especially with Random Forest) | Higher risk of overfitting, especially if not properly regularized |\n",
    "| **Speed**              | Faster due to parallel training                           | Slower due to sequential training                        |\n",
    "| **Interpretability**   | Easier to interpret (due to averaging of models)          | Harder to interpret (due to multiple dependent models)    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f4a4d",
   "metadata": {},
   "source": [
    "#### Not here\n",
    "\n",
    "**Can Gini Impurity and Entropy be used for both numerical and categorical input features, as well as for both numerical and categorical target variables?**\n",
    " - **For Input Features**:\n",
    "\n",
    "     - Yes, both Gini Impurity and Entropy can be applied to numerical and categorical input features.\n",
    "     - For numerical features, the decision tree evaluates thresholds (e.g., feature ≤ threshold) to split the data.\n",
    "     - For categorical features, it splits based on the categories (e.g., feature == 'category_A').\n",
    "\n",
    "\n",
    " - **For Target Variables**:\n",
    "\n",
    "    - No, Gini Impurity and Entropy are only used for categorical target variables.\n",
    "    - These metrics measure the \"purity\" of categorical classes (e.g., classifying into categories like 'yes' or 'no').\n",
    "    - For numerical target variables, decision trees use different metrics like Mean Squared Error (MSE) or Mean Absolute Error (MAE), not Gini Impurity or Entropy, since these are regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02675012",
   "metadata": {},
   "source": [
    "# 5. Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b2f767",
   "metadata": {},
   "source": [
    "**Boosting** is a machine learning technique, where multiple weak leaners (often decision tree) are trained sequentially, with each new model focusing on correcting the errors made by the previous models.\n",
    "\n",
    "- the goal is to combine weak learners to form a strong, accurate model\n",
    "- there are many boosting model, most popular are\n",
    "   - **AdaBoost** : Short for Adaptive Boosting\n",
    "   - **Gradient Boosting**: (some of its version are XGBoost, CatBoost, LightGBM)\n",
    "   \n",
    "**How Boosting Works**:\n",
    "- **Initial Model**: A weak model is trained on the dataset.\n",
    "- **Error Focus**: After training, the model's performance is evaluated. The instances that were predicted incorrectly are given higher importance (weights), so the next model focuses more on them.\n",
    "- **Sequential Training**: This process repeats, with each new model trying to correct the mistakes of the previous one. Models are trained sequentially, not independently.\n",
    "- **Final Prediction**: At the end, the predictions of all the models are combined (usually through weighted voting for classification or averaging for regression) to make the final prediction\n",
    "\n",
    "- **Pros of Boosting** \n",
    "  - Can significantly improve model performance by correcting errors made by weak learners\n",
    "  - Works well for both classification and regression tasks.\n",
    "  - Many variations (like XGBoost, LightGBM) include built-in regularization to prevent overfitting.\n",
    "\n",
    "**Cons of Boosting**:\n",
    "- Training can be slow since models are trained sequentially.\n",
    "- More prone to overfitting if not properly regularized.\n",
    "- Hyperparameter tuning is crucial for optimal performance and can be time-consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1986f04",
   "metadata": {},
   "source": [
    "#### Types of Boosting:\n",
    "**AdaBoost (Adaptive Boosting)**:\n",
    "   - Focuses on adjusting the weights of incorrectly predicted instances. \n",
    "   - Each subsequent model pays more attention to the errors of the previous model.\n",
    "\n",
    "**Gradient Boosting**:\n",
    "  - Sequentially builds models to minimize a loss function (e.g., mean squared error). \n",
    "  - Each new model is trained to correct the residuals (errors) of the previous model's predictions.\n",
    "\n",
    "  - **XGBoost**:\n",
    "      - An optimized version of gradient boosting that includes regularization and faster computation (parallelization). \n",
    "      - It is popular for structured/tabular data.\n",
    "  \n",
    "  - **LightGBM**:\n",
    "      - A more efficient version of gradient boosting that works particularly well with large datasets by using histogram-based algorithms and leaf-wise splits.\n",
    "\n",
    "  - **CatBoost**:\n",
    "     - A gradient boosting algorithm designed to handle categorical features effectively without requiring explicit one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4989b",
   "metadata": {},
   "source": [
    "## 5.1 AdaBoost -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744d162d",
   "metadata": {},
   "source": [
    "\n",
    "- One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfit. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.\n",
    "\n",
    "- Adaboost, sequential learning technique has some similarities with gradient descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better\n",
    "\n",
    "- there is one important drawback to this sequential learning technique: training cannot be parallelized since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4f293",
   "metadata": {},
   "source": [
    "## -----\n",
    "\n",
    "**AdaBoost Algorithm**\n",
    "  - Each instance, at initial, their weight is set to 1/m\n",
    "  - A first predicator is trained, and its weight error rate r_1 is computed on the training set\n",
    "      - Weighted error rate of the jth predictor\n",
    "          - rj = ∑ i=1 y^ j (i) ≠ y (i) m w(i) \n",
    "          - where y^ j(i) is the jth predictor’s predictionfor the i th instance\n",
    "  - the predicator weight αj is then computed using   **α j = η log 1-r j r j** \n",
    "      - where  η is the learning rate hyperparameter (defaults to 1)\n",
    "      - the more accurate the predicator is, the higher its weight will be\n",
    "      - if it is guessing randomly, its value will be close to zero\n",
    "      - if it is most oftern wrong (i.e. less accurate than random guessing), then its weight will be negative\n",
    "  - Next, the AdaBoost algorithm updates the instance weights, which boosts the weights of the misclassified instances.\n",
    "  - Then all the instance weights are normalized (i.e., divided by ∑i=1mw(i))\n",
    "  - Finally, a new predictor is trained using the updated weights, \n",
    "  - and the whole process is repeated:\n",
    "       - the new predictor’s weight is computed, \n",
    "       - the instance weights are updated, \n",
    "       - then another predictor is trained, and so on. \n",
    "   - The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855448f",
   "metadata": {},
   "source": [
    "## -----\n",
    "\n",
    "- **Scikit learn uses a multiclass version of Adaboost called SAMME**\n",
    "    - Stagewise Additive Modelling using Multiclass Exponetial loss function\n",
    "- when there are just two class, SAMME is equivalent o Adaboost\n",
    "- if predicators can estimate class probabilities (i.e. if they havea predict_proba() method) sckit learn can use a variant of SAMME called SAMME.R (R stands for \"Real\") \n",
    "   - it reilies on class probabilites rather than predications and generally performs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00220b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=30, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AdaBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=30, random_state=42)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=30, random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below code trains Adaboost classifier, based on 30 decision stump\n",
    "# A decision Stump is a decision tree with max depth =1, \n",
    "# in laymen terms, a tree composed of single decision node plus two leaf nodes\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1),\n",
    "                            n_estimators = 30,\n",
    "                            learning_rate = 0.5,\n",
    "                            random_state = 42)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141df27",
   "metadata": {},
   "source": [
    "**If your Adaboost is overfitting the training set, we can try reducing the number of estimators or more strongly reqularizing the base estimator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a45948",
   "metadata": {},
   "source": [
    "## 5.2 Gradient Boosting -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb368aac",
   "metadata": {},
   "source": [
    "- Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. \n",
    "- However, instead of tweaking the instance weights at every iteration like AdaBoost does, \n",
    "   - **this method tries to fit the new predictor to the residual errors made by the previous predictor**\n",
    "   \n",
    "   \n",
    "- Key Differences:\n",
    "  - **AdaBoost**:\n",
    "      - Increases weight on misclassified instances, focusing directly on errors in the training set.\n",
    "  - **Gradient Boosting**:\n",
    "      - Fits a new model to the residuals of the entire ensemble's predictions, aiming to minimize the overall error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc4c29a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=42)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets go through a regression example, \n",
    "# using decision tree as the base predicator, called Gradient Tree Boosting or gradient boosted regression Trees (GBRT)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.random.rand(100,1) - 0.5\n",
    "y = 3*X[:, 0] ** 2 + 0.005 * np.random.rand(100) # y = 3x^2 + gausian noise\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth = 2, random_state = 42)\n",
    "tree_reg1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d5ad64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2, random_state=43)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2, random_state=43)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=43)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will train secong DecisionTreeRegressor on the residuals error made by the first predictor\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth = 2, random_state = 43)\n",
    "\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37d5aae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=2, random_state=44)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=2, random_state=44)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2, random_state=44)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again we ill train a third regressor on residuals error made by the second predicator\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth = 2, random_state = 44)\n",
    "\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d0af06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44462706, 0.05141457, 0.64251501])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here need to go through chatgpt for understanding\n",
    "\n",
    "\n",
    "X_new = np.array([[-0.4], [0.], [0.5]])\n",
    "sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d6e9815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9556789807796691"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combined Accuracy of the First 3 Models:\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_pred_combined = sum(tree.predict(X) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "accuracy = r2_score(y, y_pred_combined)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4db4a9",
   "metadata": {},
   "source": [
    "- The accuracy of individual models in this approach isn't as relevant since each tree is trained on the residuals.\n",
    "- We should focus on the combined prediction from the models and evaluate their performance on the dataset.\n",
    "- Use a metric like R-squared to assess the final accuracy after combining the predictions from all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa147d8",
   "metadata": {},
   "source": [
    "#### GBRT \n",
    "\n",
    "- we can use Scikit-Learn’s GradientBoostingRegressor class to train GBRT ensembles more easily (there’s also a GradientBoostingClassifier class for classification).\n",
    "\n",
    "- Much like RandomForestRegressor Class it has hyperparameters to the control the ensemble training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af0f68ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3,\n",
       "                          random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth = 2, n_estimators = 3,\n",
    "                                learning_rate = 1.0, random_state = 42)\n",
    "\n",
    "gbrt.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2567d4a5",
   "metadata": {},
   "source": [
    "- If you set it to a low value, such as 0.05, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called shrinkage.\n",
    "\n",
    "## -----\n",
    "\n",
    "- **Shrinkage (also known as learning rate)** is a crucial parameter in gradient boosting ensemble methods like Gradient Boosting Machines (GBMs) and XGBoost. \n",
    "\n",
    "- It controls the contribution of each weak learner (or tree) to the final prediction, effectively slowing down the learning process to avoid overfitting and improve generalization.\n",
    "\n",
    "**What is Shrinkage in Gradient Boosting?**\n",
    "- In gradient boosting, the model is built iteratively by fitting weak learners (typically decision trees) to the residuals (errors) of the previous iteration. \n",
    "- Each iteration's tree corrects the errors of the previous ones. \n",
    "- The shrinkage (or learning rate) scales down the influence of each tree by a factor η, which reduces the impact of individual trees on the final model.\n",
    "\n",
    "The prediction for a new instance after t iterations is given by:\n",
    "\n",
    "$$\\hat{y} = \\sum_{i=1}^{t} \\eta \\cdot f_i(x)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5181f3b4",
   "metadata": {},
   "source": [
    "**Example of Shrinkage Impact:**\n",
    " - Let’s imagine we are using gradient boosting to predict house prices, and we are at the 3rd iteration (or tree).\n",
    " - Without shrinkage, the model would simply add the predictions of the 3rd tree directly to the sum of previous trees’ predictions. \n",
    " - However, with shrinkage, the contribution of each tree is scaled down by η, say 0.1:\n",
    "\n",
    "   - If the prediction from tree 3 is 10,000 (the amount it predicts should be added to the previous predictions), then the contribution with shrinkage will be: **0.1 × 10,000 = 1000**\n",
    "\n",
    "- This reduced contribution helps avoid overfitting early in the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bec75",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "- Shrinkage (learning rate) is a regularization technique in gradient boosting that scales down the contribution of each tree.\n",
    "- It helps prevent overfitting by slowing down the learning process.\n",
    "- Smaller learning rates usually require more trees, while larger learning rates may lead to quicker but less generalizable models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e0665",
   "metadata": {},
   "source": [
    "- GBRT would start to overfit if we add extra tree\n",
    "- to find the optimal number of trees, we can perform cross-validation using GridSearchCV or RandomizedSearchCV\n",
    "- another simple way, by setting **n_iter_no_change** hyperparameter to an integer value 10\n",
    "   - then GradientBoostingRegressor will automatically stop adding trees, during training, if it sees that **the last 10 trees didn't help**\n",
    "- This is simply early stopping but with a little bit of **patience**: \n",
    "  - it tolerates having no progress for a few iterations before it stops\n",
    "  \n",
    "- **If you set n_iter_no_change too low, training may stop too early and the model will underfit**. \n",
    "   - But if you set it too high, it will overfit instead\n",
    "   \n",
    "- The combination of a small learning rate and a high initial number of estimators allows the model to explore a wide range of potential solutions without quickly converging to a suboptimal one.\n",
    "- However, since the model will stop adding trees if they do not contribute to improving performance (thanks to early stopping), the final ensemble will consist of only the most beneficial trees. \n",
    "- **This helps in creating a well-balanced model that utilizes the strengths of many trees while avoiding the pitfalls of overfitting**\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d73c6f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbrt_best = GradientBoostingRegressor(max_depth = 2, \n",
    "                                      learning_rate = 0.05,\n",
    "                                      n_estimators =500,\n",
    "                                      n_iter_no_change = 10,\n",
    "                                      tol=1e-4,               # Using the default tolerance, can adjust it if we need to make the model more or less sensitive to small improvements in the loss.\n",
    "                                      random_state = 42\n",
    ")\n",
    "\n",
    "gbrt_best.fit(X,y)\n",
    "\n",
    "# finding no of estimators/trees after which there is almost no change\n",
    "gbrt_best.n_estimators_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe9e85",
   "metadata": {},
   "source": [
    "**When n_iter_no_change is set** \n",
    "  - the fit() method automatically splits the training set into a smaller training set and a validation set: \n",
    "  - this allows it to evaluate the model’s performance each time it adds a new tree. \n",
    "  - the size of the validation set is controlled by the validation_fraction hyperparameter, which is 10% by default. \n",
    "  - The tol hyperparameter determines the maximum performance improvement that still counts as negligible. It defaults to 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e51e18",
   "metadata": {},
   "source": [
    "**The GradientBoostingRegressor class also supports a subsample hyperparameter**, \n",
    "   - which specifies the fraction of training instances to be used for training each tree. \n",
    "   - For example, if subsample=0.25, \n",
    "       - then each tree is trained on 25% of the training instances, selected randomly. \n",
    "   - this technique trades a higher bias for a lower variance. \n",
    "   - It also speeds up training considerably. This is called **stochastic gradient boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142828b2",
   "metadata": {},
   "source": [
    "## 5.3 Histogram-Based Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df38db",
   "metadata": {},
   "source": [
    "- ScikitLearn have another GBRT method, optimized for larger datasets, HGB, Histogram Based Gradient Boosting\n",
    "- **it works by binning the input features, replacing with integers**\n",
    "- the numbers of bins is controlled by the max_bins hyperparameter, which defaults to **255** and **cannot be set any higher than this**.\n",
    "- Binning can greatly reduce the number of possible thresholds that the training algorithm needs to evalute\n",
    "- Moreever, working with integers makes it possible to use faster and more memory efficient data structures.\n",
    "- And the way the bins are built removes the need for sorting the features when training each tree\n",
    "\n",
    "\n",
    "### ----\n",
    " - As a result, this implementation has a computational complexity of O(b×m) instead of O(n×m×log(m)), \n",
    "    - where b is the number of bins, \n",
    "    - m is the number of training instances, \n",
    "    - and n is the number of features.\n",
    " - In practice, this means that **HGB can train hundreds of times faster than regular GBRT on large datasets**. \n",
    " - However, **binning causes a precision loss, which acts as a regularizer**: \n",
    "    - **depending on the dataset, this may help reduce overfitting, or it may cause underfitting**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c0395",
   "metadata": {},
   "source": [
    "- Sckit-Learn provides two classes for HGB\n",
    "   - HistGradientBoostingRegressor\n",
    "   - HistGradientBoostingClassifier\n",
    " - \n",
    " - similar to GradientBoostingRegressor and GradientBoostingClassifier with few notable differences\n",
    "    - Early stopping is automatically activated if the number of instances is greater than 10,000. You can turn early stopping always on or always off by setting the early_stopping hyperparameter to True or False.\n",
    "    - Subsampling is not supported. \n",
    "    - n_estimators is renamed to max_iter.\n",
    "    - The only decision tree hyperparameters that can be tweaked are max_leaf_nodes, min_samples_leaf, and max_depth.\n",
    "    \n",
    "## ----\n",
    "- The **HGB classes also have two nice features: they support both categorical features and missing values**. \n",
    "   - This simplifies preprocessing quite a bit. \n",
    "- However, the **categorical features must be represented as integers ranging from 0 to a number lower than max_bins**.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49978032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use california housing dataset \n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "hgb_reg = make_pipeline(\n",
    "make_column_transformer((OrdinalEncoder(), ['ocean_proximity']),\n",
    "                       remainder = 'passthrough'),\n",
    "    HistGradientBoostingRegressor(categorical_features = [0],\n",
    "                                  random_state = 42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef7a7b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/saajanrajak/2024 Projects/2024_05 Machine Learning/Data/california_housing.csv\")\n",
    "\n",
    "# creating Input and output vars data frames\n",
    "\n",
    "X = df.drop(['median_house_value'], axis = 1)\n",
    "y = df['median_house_value']\n",
    "\n",
    "# lets create bin for target variable,for stratifying, better split in test and train data\n",
    "\n",
    "y_binned = pd.cut(y, bins = 6)\n",
    "\n",
    "\n",
    "# Using sklearn package create train test data sets for the model\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y_binned, random_state = 122, test_size = .20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c490335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16099</th>\n",
       "      <td>-122.51</td>\n",
       "      <td>37.76</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2345.0</td>\n",
       "      <td>624.0</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>614.0</td>\n",
       "      <td>2.8448</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10610</th>\n",
       "      <td>-117.78</td>\n",
       "      <td>33.68</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>849.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>4.0187</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16933</th>\n",
       "      <td>-122.33</td>\n",
       "      <td>37.57</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>643.0</td>\n",
       "      <td>1112.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>3.6250</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>-121.72</td>\n",
       "      <td>36.81</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>3.2969</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15982</th>\n",
       "      <td>-122.47</td>\n",
       "      <td>37.76</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2941.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>1545.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>2.9899</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "16099    -122.51     37.76                43.0       2345.0           624.0   \n",
       "10610    -117.78     33.68                11.0       1994.0           477.0   \n",
       "16933    -122.33     37.57                20.0       2126.0           643.0   \n",
       "9731     -121.72     36.81                18.0       1984.0           379.0   \n",
       "15982    -122.47     37.76                52.0       2941.0           783.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "16099      1439.0       614.0         2.8448      NEAR OCEAN  \n",
       "10610       849.0       411.0         4.0187       <1H OCEAN  \n",
       "16933      1112.0       597.0         3.6250      NEAR OCEAN  \n",
       "9731       1078.0       359.0         3.2969       <1H OCEAN  \n",
       "15982      1545.0       726.0         2.9899        NEAR BAY  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccb4dc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;ordinalencoder&#x27;,\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  [&#x27;ocean_proximity&#x27;])])),\n",
       "                (&#x27;histgradientboostingregressor&#x27;,\n",
       "                 HistGradientBoostingRegressor(categorical_features=[0],\n",
       "                                               random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;columntransformer&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;ordinalencoder&#x27;,\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  [&#x27;ocean_proximity&#x27;])])),\n",
       "                (&#x27;histgradientboostingregressor&#x27;,\n",
       "                 HistGradientBoostingRegressor(categorical_features=[0],\n",
       "                                               random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">columntransformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;ordinalencoder&#x27;, OrdinalEncoder(),\n",
       "                                 [&#x27;ocean_proximity&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ordinalencoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;ocean_proximity&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OrdinalEncoder</label><div class=\"sk-toggleable__content\"><pre>OrdinalEncoder()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;longitude&#x27;, &#x27;latitude&#x27;, &#x27;housing_median_age&#x27;, &#x27;total_rooms&#x27;, &#x27;total_bedrooms&#x27;, &#x27;population&#x27;, &#x27;households&#x27;, &#x27;median_income&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingRegressor(categorical_features=[0], random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('ordinalencoder',\n",
       "                                                  OrdinalEncoder(),\n",
       "                                                  ['ocean_proximity'])])),\n",
       "                ('histgradientboostingregressor',\n",
       "                 HistGradientBoostingRegressor(categorical_features=[0],\n",
       "                                               random_state=42))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb_reg.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c824a8",
   "metadata": {},
   "source": [
    "- The whole pipeline is just as short as the imports! \n",
    "  - No need for an imputer, scaler, or a one-hot encoder, so it’s really convenient. \n",
    "- Note that categorical_features must be set to the categorical column indices (or a Boolean array).\n",
    "- Without any hyperparameter tuning, this model yields an RMSE of about 47,600, which is not too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4a44d",
   "metadata": {},
   "source": [
    "- Several other optimized implementations of gradient boosting are available in the Python ML ecosystem: \n",
    "  - in particular, XGBoost, CatBoost, and LightGBM. These libraries have been around for several years.\n",
    "  - They are all specialized for gradient boosting, their APIs are very similar to Scikit-Learn’s, and they provide many additional features, including GPU acceleration will checkout in depth later\n",
    "- Moreover, the TensorFlow Random Forests library provides optimized implementations of a variety of random forest algorithms, including plain random forests, extra-trees, GBRT, and several more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b66fd5c",
   "metadata": {},
   "source": [
    "# 6. Stacking ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6458b0",
   "metadata": {},
   "source": [
    "- it is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, \n",
    "    - **we train a model to perform this aggregation**\n",
    "       - example\n",
    "       - suppose ensemble performing a regression task on a new instance, having three models\n",
    "       - Each models (three) will predict a different value\n",
    "       - the predications used to make final predications( called **Blender or Meta Learner**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7bcf6",
   "metadata": {},
   "source": [
    "- Scenario:\n",
    "   - building a regression model to predict house prices.\n",
    "   - we have trained three different models:\n",
    "      - Decision Tree Regressor\n",
    "      - Random Forest Regressor\n",
    "      - Linear Regression\n",
    "   - Each of these models will make their own predictions for a new house based on features like the size of the house, number of rooms, location, etc.\n",
    "\n",
    "#### Step-by-Step Process:\n",
    "**Step 1: Predictions from Individual Models**\n",
    "   - Suppose we input the features of a new house into all three models:\n",
    "\n",
    "       - Decision Tree Regressor predicts: 200,000\n",
    "       - Random Forest Regressor predicts: 210,000\n",
    "       - Linear Regression predicts: 195,000\n",
    "       - Now, you have three different predictions.\n",
    "\n",
    "**Step 2: Meta-Learner (or Blender) for Final Prediction**\n",
    "   - The blender or meta-learner will take these three predictions as input and learn how to combine them to produce a final prediction.\n",
    "\n",
    "   - The blender could be a simple Linear Regression model trained to combine these outputs:\n",
    "\n",
    "**Step 3 : Let’s assume the blender was trained and learned that:**\n",
    "\n",
    "   - 30% weight should be given to the Decision Tree prediction,\n",
    "   - 50% weight to the Random Forest prediction,\n",
    "   - 20% weight to the Linear Regression prediction.\n",
    "   \n",
    " \n",
    "**Step 4: Final Prediction Calculation**\n",
    "  - The blender (meta-learner) will use these weights to calculate the final prediction:\n",
    "  - Final Prediction=(0.3×200,000)+(0.5×210,000)+(0.2×195,000)\n",
    "  - Final Prediction=60,000+105,000+39,000=204,000\n",
    "\n",
    "So, the blender/meta-learner makes a final prediction of $204,000 for the house price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac3067",
   "metadata": {},
   "source": [
    "#### Not here, just for knowledge (jfk)\n",
    "Example (5-fold CV with RandomForestRegressor):\n",
    "  - Fold 1: Train on Folds 2–5, predict on Fold 1.\n",
    "  - Fold 2: Train on Folds 1, 3–5, predict on Fold 2.\n",
    "  - Fold 3: Train on Folds 1, 2, 4, 5, predict on Fold 3.\n",
    "  - Fold 4: Train on Folds 1–3, 5, predict on Fold 4.\n",
    "  - Fold 5: Train on Folds 1–4, predict on Fold 5.\n",
    "  \n",
    "**At the end of the 5 folds, you will have one prediction for each instance in the training data from this model**.\n",
    "\n",
    "- This process is repeated for each model in the ensemble, and these predictions become the input features for training the blender."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60083ee0",
   "metadata": {},
   "source": [
    "**Multilayer Stacking Ensemble**\n",
    "\n",
    "- It is actually possible to train several different blenders this way (e.g., one using linear regression, another using random forest regression) to get a whole layer of blenders, and then add another blender on top of that to produce the final prediction, \n",
    "- may be able to squeeze out a few more drops of performance by doing this, but it will cost us in both training time and system complexity.\n",
    "- Scikit-Learn provides two classes for stacking ensembles: StackingClassifier and StackingRegressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47fa22a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples = 500, noise = 0.30, random_state = 42)\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88d73b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(cv=5,\n",
       "                   estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                               (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                               (&#x27;svc&#x27;, SVC(probability=True, random_state=42))],\n",
       "                   final_estimator=RandomForestClassifier(random_state=43))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(cv=5,\n",
       "                   estimators=[(&#x27;lr&#x27;, LogisticRegression(random_state=42)),\n",
       "                               (&#x27;rf&#x27;, RandomForestClassifier(random_state=42)),\n",
       "                               (&#x27;svc&#x27;, SVC(probability=True, random_state=42))],\n",
       "                   final_estimator=RandomForestClassifier(random_state=43))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True, random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=43)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingClassifier(cv=5,\n",
       "                   estimators=[('lr', LogisticRegression(random_state=42)),\n",
       "                               ('rf', RandomForestClassifier(random_state=42)),\n",
       "                               ('svc', SVC(probability=True, random_state=42))],\n",
       "                   final_estimator=RandomForestClassifier(random_state=43))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(estimators = [('lr', LogisticRegression(random_state = 42)),\n",
    "                                                ('rf', RandomForestClassifier(random_state = 42)),\n",
    "                                                ('svc', SVC(probability = True, random_state = 42))\n",
    "                                               ],\n",
    "                                  final_estimator = RandomForestClassifier(random_state = 43),\n",
    "                                  cv = 5 #number of cross Vlaidation Folds\n",
    "                                 )\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a90a6",
   "metadata": {},
   "source": [
    "# 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e4513",
   "metadata": {},
   "source": [
    "- ensemble methods are versatile, powerful, and fairly simple to use. \n",
    "- Random forests, AdaBoost, and GBRT are among the first models we should test for most machine learning tasks \n",
    "  - and they particularly shine with heterogeneous tabular data.\n",
    "  - Moreover, as they require very little preprocessing, \n",
    "- these are great for getting a prototype up and running quickly. \n",
    "- Lastly, ensemble methods like voting classifiers and stacking classifiers can help push your system’s performance to its limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e17b8af",
   "metadata": {},
   "source": [
    "# Happy Learning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
